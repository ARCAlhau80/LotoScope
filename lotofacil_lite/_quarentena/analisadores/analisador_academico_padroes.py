"""
üî¨ ANALISADOR ACAD√äMICO DE PADR√ïES LOTOF√ÅCIL
=============================================
Sistema completo para descoberta de padr√µes estat√≠sticos usando m√©todos acad√™micos
"""

import pyodbc

# üöÄ SISTEMA DE OTIMIZA√á√ÉO DE BANCO
try:
    from database_optimizer import DatabaseOptimizer
    _db_optimizer = DatabaseOptimizer()
except ImportError:
    _db_optimizer = None

import numpy as np
import pandas as pd
import scipy.stats as stats
from scipy.stats import chi2_contingency, pearsonr, spearmanr
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from collections import Counter, defaultdict
from datetime import datetime, timedelta
import json
import os

  # Usa conex√£o cached para performance
  self.conn = _conn_cache.get_connection()
class AnalisadorPadroesAcademico:
    """
    Analisador acad√™mico para descoberta de padr√µes na Lotof√°cil
    Implementa m√©todos estat√≠sticos rigorosos para an√°lise de dados
    """
    
    def __init__(self):
        self.conn = None
        self.dados = None
        self.resultados_analise = {}
        
    def conectar_banco(self):
        """Conecta ao banco SQL Server"""
        try:
            server = 'DESKTOP-K6JPBDS'
            database = 'LOTOFACIL'
            trusted_connection = 'yes'
            connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection={trusted_connection};'
            # Usa conex√£o cached para performance
            self.conn = _conn_cache.get_connection()
            print("‚úÖ Conex√£o estabelecida com sucesso")
            return True
        except Exception as e:
            print(f"‚ùå Erro de conex√£o: {e}")
            return False
    
    def carregar_dados_completos(self):
        """Carrega todos os dados hist√≥ricos para an√°lise"""
        if not self.conn:
            print("‚ùå Conex√£o n√£o estabelecida")
            return False
            
        try:
            query = """
            SELECT 
                Concurso, Data_Sorteio,
                N1, N2, N3, N4, N5, N6, N7, N8, N9, N10, N11, N12, N13, N14, N15,
                QtdePrimos, QtdeFibonacci, QtdeImpares, SomaTotal,
                Quintil1, Quintil2, Quintil3, Quintil4, Quintil5,
                QtdeGaps, QtdeRepetidos, SEQ, DistanciaExtremos, ParesSequencia,
                QtdeMultiplos3, ParesSaltados, Faixa_Baixa, Faixa_Media, Faixa_Alta,
                RepetidosMesmaPosicao, menor_que_ultimo, maior_que_ultimo, igual_ao_ultimo
            FROM RESULTADOS_INT
            ORDER BY Concurso
            """
            
            self.dados = pd.read_sql(query, self.conn)
            print(f"‚úÖ Carregados {len(self.dados)} concursos para an√°lise")
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar dados: {e}")
            return False
    
    def analise_frequencias_numeros(self):
        """
        üìä AN√ÅLISE 1: Frequ√™ncias e Distribui√ß√µes de N√∫meros
        """
        print("\nüîç AN√ÅLISE DE FREQU√äNCIAS DE N√öMEROS...")
        
        # Coletar todos os n√∫meros sorteados
        numeros_colunas = [f'N{i}' for i in range(1, 16)]
        todos_numeros = []
        
        for _, row in self.dados.iterrows():
            for col in numeros_colunas:
                todos_numeros.append(row[col])
        
        # An√°lise de frequ√™ncia
        freq_numeros = Counter(todos_numeros)
        freq_esperada = len(todos_numeros) / 25  # Frequ√™ncia esperada se fosse uniforme
        
        # Teste chi-quadrado para uniformidade
        frequencias_observadas = [freq_numeros[i] for i in range(1, 26)]
        chi2_stat, p_value = stats.chisquare(frequencias_observadas)
        
        # Identificar n√∫meros "quentes" e "frios"
        freq_media = np.mean(frequencias_observadas)
        freq_std = np.std(frequencias_observadas)
        
        numeros_quentes = [i for i in range(1, 26) if freq_numeros[i] > freq_media + freq_std]
        numeros_frios = [i for i in range(1, 26) if freq_numeros[i] < freq_media - freq_std]
        
        # Coeficiente de varia√ß√£o
        cv = freq_std / freq_media
        
        resultado = {
            'frequencias': dict(freq_numeros),
            'freq_esperada': freq_esperada,
            'chi2_uniformidade': {'estatistica': chi2_stat, 'p_valor': p_value},
            'numeros_quentes': numeros_quentes,
            'numeros_frios': numeros_frios,
            'coeficiente_variacao': cv,
            'interpretacao': self._interpretar_frequencias(p_value, cv, numeros_quentes, numeros_frios)
        }
        
        self.resultados_analise['frequencias_numeros'] = resultado
        return resultado
    
    def analise_correlacoes_temporais(self):
        """
        üìà AN√ÅLISE 2: Correla√ß√µes Temporais e Tend√™ncias
        """
        print("\nüîç AN√ÅLISE DE CORRELA√á√ïES TEMPORAIS...")
        
        # An√°lise de autocorrela√ß√£o para cada campo
        campos_numericos = ['SomaTotal', 'QtdePrimos', 'QtdeFibonacci', 'QtdeImpares', 
                           'QtdeGaps', 'QtdeRepetidos', 'SEQ', 'DistanciaExtremos']
        
        correlacoes = {}
        tendencias = {}
        
        for campo in campos_numericos:
            serie = self.dados[campo].values
            
            # Autocorrela√ß√£o com lag 1
            if len(serie) > 1:
                autocorr = pearsonr(serie[:-1], serie[1:])[0]
                correlacoes[campo] = autocorr
            
            # Tend√™ncia temporal usando regress√£o linear
            x = np.arange(len(serie))
            slope, intercept, r_value, p_value, std_err = stats.linregress(x, serie)
            
            tendencias[campo] = {
                'slope': slope,
                'r_squared': r_value**2,
                'p_valor': p_value,
                'significativa': p_value < 0.05
            }
        
        # Matriz de correla√ß√£o entre diferentes campos
        df_campos = self.dados[campos_numericos]
        matriz_correlacao = df_campos.corr()
        
        # Identificar correla√ß√µes fortes (|r| > 0.5)
        correlacoes_fortes = []
        for i in range(len(matriz_correlacao.columns)):
            for j in range(i+1, len(matriz_correlacao.columns)):
                corr_val = matriz_correlacao.iloc[i, j]
                if abs(corr_val) > 0.5:
                    correlacoes_fortes.append({
                        'campo1': matriz_correlacao.columns[i],
                        'campo2': matriz_correlacao.columns[j],
                        'correlacao': corr_val
                    })
        
        resultado = {
            'autocorrelacoes': correlacoes,
            'tendencias_temporais': tendencias,
            'matriz_correlacao': matriz_correlacao.to_dict(),
            'correlacoes_fortes': correlacoes_fortes,
            'interpretacao': self._interpretar_correlacoes(correlacoes, tendencias, correlacoes_fortes)
        }
        
        self.resultados_analise['correlacoes_temporais'] = resultado
        return resultado
    
    def analise_sazonalidade_ciclos(self):
        """
        üîÑ AN√ÅLISE 3: Sazonalidade e Detec√ß√£o de Ciclos
        """
        print("\nüîç AN√ÅLISE DE SAZONALIDADE E CICLOS...")
        
        # Converter data para datetime se necess√°rio
        self.dados['Data_Sorteio'] = pd.to_datetime(self.dados['Data_Sorteio'])
        self.dados['DiaSemana'] = self.dados['Data_Sorteio'].dt.dayofweek
        self.dados['Mes'] = self.dados['Data_Sorteio'].dt.month
        self.dados['Ano'] = self.dados['Data_Sorteio'].dt.year
        
        # An√°lise por dia da semana
        analise_dia_semana = {}
        for campo in ['SomaTotal', 'QtdePrimos', 'QtdeImpares']:
            dados_por_dia = self.dados.groupby('DiaSemana')[campo].agg(['mean', 'std', 'count'])
            
            # Teste ANOVA para diferen√ßas significativas
            grupos = [self.dados[self.dados['DiaSemana'] == dia][campo].values 
                     for dia in range(7)]
            f_stat, p_value = stats.f_oneway(*grupos)
            
            analise_dia_semana[campo] = {
                'estatisticas_por_dia': dados_por_dia.to_dict(),
                'anova': {'f_estatistica': f_stat, 'p_valor': p_value}
            }
        
        # An√°lise por m√™s
        analise_mensal = {}
        for campo in ['SomaTotal', 'QtdePrimos', 'QtdeImpares']:
            dados_por_mes = self.dados.groupby('Mes')[campo].agg(['mean', 'std', 'count'])
            
            # Teste ANOVA para diferen√ßas mensais
            grupos = [self.dados[self.dados['Mes'] == mes][campo].values 
                     for mes in range(1, 13)]
            f_stat, p_value = stats.f_oneway(*grupos)
            
            analise_mensal[campo] = {
                'estatisticas_por_mes': dados_por_mes.to_dict(),
                'anova': {'f_estatistica': f_stat, 'p_valor': p_value}
            }
        
        # Detec√ß√£o de ciclos usando FFT
        ciclos_detectados = {}
        for campo in ['SomaTotal', 'QtdePrimos', 'QtdeImpares']:
            serie = self.dados[campo].values
            
            # Remover tend√™ncia
            detrended = stats.detrend(serie)
            
            # FFT para detectar periodicidades
            fft = np.fft.fft(detrended)
            freqs = np.fft.fftfreq(len(detrended))
            
            # Encontrar picos significativos
            magnitude = np.abs(fft)
            picos_indices = np.where(magnitude > np.percentile(magnitude, 95))[0]
            
            ciclos = []
            for idx in picos_indices:
                if freqs[idx] > 0:  # Apenas frequ√™ncias positivas
                    periodo = 1 / freqs[idx]
                    if 2 <= periodo <= len(serie) / 4:  # Per√≠odos razo√°veis
                        ciclos.append({
                            'periodo': periodo,
                            'intensidade': magnitude[idx]
                        })
            
            ciclos_detectados[campo] = sorted(ciclos, key=lambda x: x['intensidade'], reverse=True)[:5]
        
        resultado = {
            'analise_dia_semana': analise_dia_semana,
            'analise_mensal': analise_mensal,
            'ciclos_detectados': ciclos_detectados,
            'interpretacao': self._interpretar_sazonalidade(analise_dia_semana, analise_mensal, ciclos_detectados)
        }
        
        self.resultados_analise['sazonalidade_ciclos'] = resultado
        return resultado
    
    def analise_deteccao_anomalias(self):
        """
        üö® AN√ÅLISE 4: Detec√ß√£o de Anomalias e Outliers
        """
        print("\nüîç AN√ÅLISE DE DETEC√á√ÉO DE ANOMALIAS...")
        
        campos_analise = ['SomaTotal', 'QtdePrimos', 'QtdeFibonacci', 'QtdeImpares', 
                         'QtdeGaps', 'QtdeRepetidos', 'SEQ', 'DistanciaExtremos']
        
        anomalias_detectadas = {}
        
        for campo in campos_analise:
            valores = self.dados[campo].values
            
            # M√©todo 1: Z-Score
            z_scores = np.abs(stats.zscore(valores))
            outliers_zscore = np.where(z_scores > 3)[0]
            
            # M√©todo 2: IQR (Interquartile Range)
            Q1 = np.percentile(valores, 25)
            Q3 = np.percentile(valores, 75)
            IQR = Q3 - Q1
            limite_inferior = Q1 - 1.5 * IQR
            limite_superior = Q3 + 1.5 * IQR
            outliers_iqr = np.where((valores < limite_inferior) | (valores > limite_superior))[0]
            
            # M√©todo 3: Isolation Forest (algoritmo de ML)
            from sklearn.ensemble import IsolationForest
            iso_forest = IsolationForest(contamination=0.05, random_state=42)
            outliers_iso = np.where(iso_forest.fit_predict(valores.reshape(-1, 1)) == -1)[0]
            
            # Combinar detec√ß√µes
            outliers_combinados = list(set(outliers_zscore) | set(outliers_iqr) | set(outliers_iso))
            
            # Detalhes dos outliers
            outliers_detalhes = []
            for idx in outliers_combinados:
                outliers_detalhes.append({
                    'concurso': self.dados.iloc[idx]['Concurso'],
                    'valor': valores[idx],
                    'z_score': z_scores[idx],
                    'metodos_detectaram': {
                        'zscore': idx in outliers_zscore,
                        'iqr': idx in outliers_iqr,
                        'isolation_forest': idx in outliers_iso
                    }
                })
            
            anomalias_detectadas[campo] = {
                'quantidade_outliers': len(outliers_combinados),
                'percentual': len(outliers_combinados) / len(valores) * 100,
                'limites_iqr': {'inferior': limite_inferior, 'superior': limite_superior},
                'outliers_detalhes': outliers_detalhes[:10]  # Top 10 outliers
            }
        
        # An√°lise de concursos com m√∫ltiplas anomalias
        concursos_anomalos = defaultdict(list)
        for campo, info in anomalias_detectadas.items():
            for outlier in info['outliers_detalhes']:
                concursos_anomalos[outlier['concurso']].append(campo)
        
        concursos_multiplas_anomalias = {
            concurso: campos for concurso, campos in concursos_anomalos.items() 
            if len(campos) > 1
        }
        
        resultado = {
            'anomalias_por_campo': anomalias_detectadas,
            'concursos_multiplas_anomalias': concursos_multiplas_anomalias,
            'interpretacao': self._interpretar_anomalias(anomalias_detectadas, concursos_multiplas_anomalias)
        }
        
        self.resultados_analise['deteccao_anomalias'] = resultado
        return resultado
    
    def analise_clustering_padroes(self):
        """
        üéØ AN√ÅLISE 5: Clustering e Agrupamento de Padr√µes
        """
        print("\nüîç AN√ÅLISE DE CLUSTERING E PADR√ïES...")
        
        # Preparar dados para clustering
        campos_clustering = ['SomaTotal', 'QtdePrimos', 'QtdeFibonacci', 'QtdeImpares', 
                           'QtdeGaps', 'QtdeRepetidos', 'SEQ', 'DistanciaExtremos',
                           'Faixa_Baixa', 'Faixa_Media', 'Faixa_Alta']
        
        dados_clustering = self.dados[campos_clustering].copy()
        
        # Normaliza√ß√£o dos dados
        scaler = StandardScaler()
        dados_normalizados = scaler.fit_transform(dados_clustering)
        
        # Determinar n√∫mero √≥timo de clusters usando m√©todo do cotovelo
        inercias = []
        K_range = range(2, 11)
        
        for k in K_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(dados_normalizados)
            inercias.append(kmeans.inertia_)
        
        # Encontrar o "cotovelo" 
        # M√©todo simples: maior redu√ß√£o percentual
        reducoes = []
        for i in range(1, len(inercias)):
            reducao = (inercias[i-1] - inercias[i]) / inercias[i-1] * 100
            reducoes.append(reducao)
        
        k_otimo = K_range[np.argmax(reducoes) + 1]
        
        # Clustering final
        kmeans_final = KMeans(n_clusters=k_otimo, random_state=42, n_init=10)
        clusters = kmeans_final.fit_predict(dados_normalizados)
        
        # An√°lise dos clusters
        self.dados['Cluster'] = clusters
        analise_clusters = {}
        
        for cluster_id in range(k_otimo):
            mask = clusters == cluster_id
            cluster_data = dados_clustering[mask]
            
            analise_clusters[cluster_id] = {
                'tamanho': int(np.sum(mask)),
                'percentual': float(np.sum(mask) / len(dados_clustering) * 100),
                'caracteristicas': {
                    campo: {
                        'media': float(cluster_data[campo].mean()),
                        'std': float(cluster_data[campo].std()),
                        'min': float(cluster_data[campo].min()),
                        'max': float(cluster_data[campo].max())
                    } for campo in campos_clustering
                },
                'concursos_exemplo': self.dados[mask]['Concurso'].head(5).tolist()
            }
        
        # PCA para visualiza√ß√£o
        pca = PCA(n_components=2)
        dados_pca = pca.fit_transform(dados_normalizados)
        
        variancia_explicada = pca.explained_variance_ratio_
        
        resultado = {
            'k_otimo': k_otimo,
            'inercias_por_k': dict(zip(K_range, inercias)),
            'analise_clusters': analise_clusters,
            'pca_variancia_explicada': variancia_explicada.tolist(),
            'silhueta_score': float(self._calcular_silhueta(dados_normalizados, clusters)),
            'interpretacao': self._interpretar_clustering(analise_clusters, k_otimo)
        }
        
        self.resultados_analise['clustering_padroes'] = resultado
        return resultado
    
    def analise_entropia_aleatoriedade(self):
        """
        üé≤ AN√ÅLISE 6: Entropia e Medidas de Aleatoriedade
        """
        print("\nüîç AN√ÅLISE DE ENTROPIA E ALEATORIEDADE...")
        
        # An√°lise de entropia para sequ√™ncias de n√∫meros
        numeros_colunas = [f'N{i}' for i in range(1, 16)]
        
        # Entropia de Shannon para cada posi√ß√£o
        entropias_posicao = {}
        for i, col in enumerate(numeros_colunas, 1):
            valores = self.dados[col].values
            valor_counts = Counter(valores)
            total = len(valores)
            
            # Calcular entropia de Shannon
            entropia = -sum((count/total) * np.log2(count/total) for count in valor_counts.values())
            entropia_maxima = np.log2(25)  # M√°xima entropia poss√≠vel (25 n√∫meros)
            entropia_normalizada = entropia / entropia_maxima
            
            entropias_posicao[f'posicao_{i}'] = {
                'entropia': entropia,
                'entropia_normalizada': entropia_normalizada,
                'uniformidade': entropia_normalizada  # Quanto mais pr√≥ximo de 1, mais uniforme
            }
        
        # An√°lise de runs (sequ√™ncias)
        def calcular_runs_test(sequencia):
            """Teste de runs para aleatoriedade"""
            # Converter para bin√°rio baseado na mediana
            mediana = np.median(sequencia)
            binario = [1 if x > mediana else 0 for x in sequencia]
            
            # Contar runs
            runs = 1
            for i in range(1, len(binario)):
                if binario[i] != binario[i-1]:
                    runs += 1
            
            # Calcular estat√≠stica do teste
            n1 = sum(binario)
            n2 = len(binario) - n1
            
            if n1 == 0 or n2 == 0:
                return None
            
            media_runs = (2 * n1 * n2) / (n1 + n2) + 1
            var_runs = (2 * n1 * n2 * (2 * n1 * n2 - n1 - n2)) / ((n1 + n2)**2 * (n1 + n2 - 1))
            
            if var_runs <= 0:
                return None
            
            z_score = (runs - media_runs) / np.sqrt(var_runs)
            p_valor = 2 * (1 - stats.norm.cdf(abs(z_score)))
            
            return {
                'runs_observados': runs,
                'runs_esperados': media_runs,
                'z_score': z_score,
                'p_valor': p_valor,
                'aleatorio': p_valor > 0.05
            }
        
        # Aplicar teste de runs em diferentes campos
        testes_runs = {}
        campos_teste = ['SomaTotal', 'QtdePrimos', 'QtdeImpares', 'QtdeGaps']
        
        for campo in campos_teste:
            resultado_runs = calcular_runs_test(self.dados[campo].values)
            if resultado_runs:
                testes_runs[campo] = resultado_runs
        
        # An√°lise de autocorrela√ß√£o para detectar padr√µes
        autocorrelacoes = {}
        for campo in campos_teste:
            serie = self.dados[campo].values
            autocorr_lags = []
            
            for lag in range(1, min(20, len(serie)//4)):
                if len(serie) > lag:
                    corr, p_val = pearsonr(serie[:-lag], serie[lag:])
                    autocorr_lags.append({
                        'lag': lag,
                        'correlacao': corr,
                        'p_valor': p_val,
                        'significativa': p_val < 0.05
                    })
            
            autocorrelacoes[campo] = autocorr_lags
        
        # Teste de Ljung-Box para autocorrela√ß√£o serial
        ljung_box_resultados = {}
        for campo in campos_teste:
            try:
                from statsmodels.stats.diagnostic import acorr_ljungbox
                resultado_lb = acorr_ljungbox(self.dados[campo].values, lags=10, return_df=True)
                ljung_box_resultados[campo] = {
                    'estatistica': resultado_lb['lb_stat'].iloc[-1],
                    'p_valor': resultado_lb['lb_pvalue'].iloc[-1],
                    'aleatorio': resultado_lb['lb_pvalue'].iloc[-1] > 0.05
                }
            except:
                ljung_box_resultados[campo] = None
        
        resultado = {
            'entropias_posicao': entropias_posicao,
            'testes_runs': testes_runs,
            'autocorrelacoes': autocorrelacoes,
            'ljung_box': ljung_box_resultados,
            'interpretacao': self._interpretar_aleatoriedade(entropias_posicao, testes_runs, ljung_box_resultados)
        }
        
        self.resultados_analise['entropia_aleatoriedade'] = resultado
        return resultado
    
    def _calcular_silhueta(self, dados, labels):
        """Calcula o coeficiente de silhueta para avaliar qualidade do clustering"""
        try:
            from sklearn.metrics import silhouette_score
            return silhouette_score(dados, labels)
        except:
            return 0.0
    
    def _interpretar_frequencias(self, p_value, cv, numeros_quentes, numeros_frios):
        """Interpreta os resultados da an√°lise de frequ√™ncias"""
        interpretacao = []
        
        if p_value < 0.05:
            interpretacao.append("üî• DESVIO SIGNIFICATIVO da distribui√ß√£o uniforme detectado")
        else:
            interpretacao.append("‚úÖ Distribui√ß√£o pr√≥xima do esperado para sorteio aleat√≥rio")
        
        if cv > 0.1:
            interpretacao.append(f"üìä Alta variabilidade nas frequ√™ncias (CV={cv:.3f})")
        else:
            interpretacao.append(f"üìä Baixa variabilidade nas frequ√™ncias (CV={cv:.3f})")
        
        if numeros_quentes:
            interpretacao.append(f"üî• N√∫meros 'quentes': {numeros_quentes}")
        
        if numeros_frios:
            interpretacao.append(f"‚ùÑÔ∏è N√∫meros 'frios': {numeros_frios}")
        
        return interpretacao
    
    def _interpretar_correlacoes(self, autocorr, tendencias, corr_fortes):
        """Interpreta os resultados da an√°lise de correla√ß√µes"""
        interpretacao = []
        
        # Autocorrela√ß√µes significativas
        autocorr_significativas = [campo for campo, valor in autocorr.items() if abs(valor) > 0.1]
        if autocorr_significativas:
            interpretacao.append(f"üîÑ Autocorrela√ß√£o detectada em: {autocorr_significativas}")
        
        # Tend√™ncias significativas
        tendencias_sig = [campo for campo, info in tendencias.items() if info['significativa']]
        if tendencias_sig:
            interpretacao.append(f"üìà Tend√™ncias temporais em: {tendencias_sig}")
        
        # Correla√ß√µes fortes
        if corr_fortes:
            interpretacao.append(f"üîó {len(corr_fortes)} correla√ß√µes fortes detectadas")
            for corr in corr_fortes[:3]:
                interpretacao.append(f"   ‚Ä¢ {corr['campo1']} ‚Üî {corr['campo2']}: r={corr['correlacao']:.3f}")
        
        return interpretacao
    
    def _interpretar_sazonalidade(self, dia_semana, mensal, ciclos):
        """Interpreta os resultados da an√°lise de sazonalidade"""
        interpretacao = []
        
        # Verificar signific√¢ncia nos dias da semana
        sig_dia = [campo for campo, info in dia_semana.items() if info['anova']['p_valor'] < 0.05]
        if sig_dia:
            interpretacao.append(f"üìÖ Efeito dia da semana significativo em: {sig_dia}")
        
        # Verificar signific√¢ncia mensal
        sig_mes = [campo for campo, info in mensal.items() if info['anova']['p_valor'] < 0.05]
        if sig_mes:
            interpretacao.append(f"üóìÔ∏è Efeito sazonal mensal em: {sig_mes}")
        
        # Ciclos detectados
        for campo, ciclos_campo in ciclos.items():
            if ciclos_campo:
                ciclo_principal = ciclos_campo[0]
                interpretacao.append(f"üîÑ {campo}: ciclo de {ciclo_principal['periodo']:.1f} sorteios")
        
        return interpretacao
    
    def _interpretar_anomalias(self, anomalias, multiplas):
        """Interpreta os resultados da detec√ß√£o de anomalias"""
        interpretacao = []
        
        # Campos com mais anomalias
        campos_ordenados = sorted(anomalias.items(), 
                                key=lambda x: x[1]['percentual'], reverse=True)
        
        campo_mais_anomalo = campos_ordenados[0]
        interpretacao.append(f"üö® {campo_mais_anomalo[0]}: {campo_mais_anomalo[1]['percentual']:.1f}% de outliers")
        
        # Concursos com m√∫ltiplas anomalias
        if multiplas:
            interpretacao.append(f"‚ö†Ô∏è {len(multiplas)} concursos com m√∫ltiplas anomalias")
            concurso_mais_anomalo = max(multiplas.items(), key=lambda x: len(x[1]))
            interpretacao.append(f"   ‚Ä¢ Concurso {concurso_mais_anomalo[0]}: anomalias em {len(concurso_mais_anomalo[1])} campos")
        
        return interpretacao
    
    def _interpretar_clustering(self, clusters, k_otimo):
        """Interpreta os resultados do clustering"""
        interpretacao = []
        
        interpretacao.append(f"üéØ {k_otimo} padr√µes distintos identificados")
        
        # Cluster maior
        cluster_maior = max(clusters.items(), key=lambda x: x[1]['tamanho'])
        interpretacao.append(f"üìä Padr√£o dominante: Cluster {cluster_maior[0]} ({cluster_maior[1]['percentual']:.1f}%)")
        
        # Caracter√≠sticas distintivas
        for cluster_id, info in clusters.items():
            if info['percentual'] > 20:  # Clusters significativos
                caracteristicas = []
                for campo, stats in info['caracteristicas'].items():
                    if stats['std'] > 0:  # Evitar divis√£o por zero
                        cv = stats['std'] / abs(stats['media'])
                        if cv < 0.3:  # Baixa variabilidade = caracter√≠stica distintiva
                            caracteristicas.append(campo)
                
                if caracteristicas:
                    interpretacao.append(f"   ‚Ä¢ Cluster {cluster_id}: caracterizado por {caracteristicas[:2]}")
        
        return interpretacao
    
    def _interpretar_aleatoriedade(self, entropias, runs, ljung_box):
        """Interpreta os resultados da an√°lise de aleatoriedade"""
        interpretacao = []
        
        # Entropia m√©dia
        entropias_valores = [info['entropia_normalizada'] for info in entropias.values()]
        entropia_media = np.mean(entropias_valores)
        
        if entropia_media > 0.9:
            interpretacao.append(f"üé≤ Alta aleatoriedade: entropia m√©dia = {entropia_media:.3f}")
        elif entropia_media > 0.7:
            interpretacao.append(f"üìä Aleatoriedade moderada: entropia m√©dia = {entropia_media:.3f}")
        else:
            interpretacao.append(f"‚ö†Ô∏è Baixa aleatoriedade: entropia m√©dia = {entropia_media:.3f}")
        
        # Testes de runs
        runs_aleatorios = [campo for campo, info in runs.items() if info and info['aleatorio']]
        if runs_aleatorios:
            interpretacao.append(f"‚úÖ Teste de runs: {len(runs_aleatorios)}/{len(runs)} campos aleat√≥rios")
        
        # Ljung-Box
        if ljung_box:
            lb_aleatorios = [campo for campo, info in ljung_box.items() 
                           if info and info['aleatorio']]
            interpretacao.append(f"‚úÖ Ljung-Box: {len(lb_aleatorios)}/{len(ljung_box)} campos sem autocorrela√ß√£o")
        
        return interpretacao
    
    def gerar_relatorio_completo(self):
        """
        üìã Gera relat√≥rio completo de todas as an√°lises
        """
        print("\nüìã GERANDO RELAT√ìRIO COMPLETO...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        nome_arquivo = f"relatorio_analise_academica_{timestamp}.json"
        
        relatorio = {
            'timestamp': timestamp,
            'total_concursos_analisados': len(self.dados),
            'periodo': {
                'inicio': int(self.dados['Concurso'].min()),
                'fim': int(self.dados['Concurso'].max())
            },
            'analises_realizadas': self.resultados_analise,
            'resumo_executivo': self._gerar_resumo_executivo()
        }
        
        # Salvar JSON
        with open(nome_arquivo, 'w', encoding='utf-8') as f:
            json.dump(relatorio, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"‚úÖ Relat√≥rio salvo: {nome_arquivo}")
        return nome_arquivo
    
    def _gerar_resumo_executivo(self):
        """Gera resumo executivo das descobertas principais"""
        resumo = {
            'principais_descobertas': [],
            'recomendacoes': [],
            'nivel_aleatoriedade': 'desconhecido',
            'padroes_significativos': []
        }
        
        # Analisar cada resultado
        for tipo_analise, resultado in self.resultados_analise.items():
            if 'interpretacao' in resultado:
                resumo['principais_descobertas'].extend(resultado['interpretacao'])
        
        # Classificar n√≠vel de aleatoriedade geral
        if 'entropia_aleatoriedade' in self.resultados_analise:
            entropia_info = self.resultados_analise['entropia_aleatoriedade']
            if entropia_info.get('entropias_posicao'):
                entropias = [info['entropia_normalizada'] 
                           for info in entropia_info['entropias_posicao'].values()]
                entropia_media = np.mean(entropias)
                
                if entropia_media > 0.9:
                    resumo['nivel_aleatoriedade'] = 'alto'
                elif entropia_media > 0.7:
                    resumo['nivel_aleatoriedade'] = 'moderado'
                else:
                    resumo['nivel_aleatoriedade'] = 'baixo'
        
        # Recomenda√ß√µes baseadas nos achados
        resumo['recomendacoes'] = [
            "Monitorar continuamente os padr√µes identificados",
            "Validar descobertas com an√°lises futuras",
            "Considerar fatores externos n√£o mensurados",
            "Aplicar m√©todos de valida√ß√£o cruzada"
        ]
        
        return resumo
    
    def executar_analise_completa(self):
        """
        üöÄ Executa toda a su√≠te de an√°lises acad√™micas
        """
        print("üî¨ INICIANDO AN√ÅLISE ACAD√äMICA COMPLETA...")
        print("=" * 60)
        
        if not self.conectar_banco():
            return False
        
        if not self.carregar_dados_completos():
            return False
        
        # Executar todas as an√°lises
        analises = [
            self.analise_frequencias_numeros,
            self.analise_correlacoes_temporais,
            self.analise_sazonalidade_ciclos,
            self.analise_deteccao_anomalias,
            self.analise_clustering_padroes,
            self.analise_entropia_aleatoriedade
        ]
        
        for i, analise in enumerate(analises, 1):
            try:
                print(f"\nüìä Executando an√°lise {i}/{len(analises)}...")
                analise()
                print(f"‚úÖ An√°lise {i} conclu√≠da")
            except Exception as e:
                print(f"‚ùå Erro na an√°lise {i}: {e}")
        
        # Gerar relat√≥rio
        arquivo_relatorio = self.gerar_relatorio_completo()
        
        print("\n" + "=" * 60)
        print("üéâ AN√ÅLISE ACAD√äMICA COMPLETA FINALIZADA!")
        print(f"üìÑ Relat√≥rio: {arquivo_relatorio}")
        
        return arquivo_relatorio

if __name__ == "__main__":
    analisador = AnalisadorPadroesAcademico()
    analisador.executar_analise_completa()