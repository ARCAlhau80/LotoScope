======================================================================
ü§ñ RELAT√ìRIO DE APRENDIZADO COM MACHINE LEARNING (10 ALGORITMOS)
======================================================================

üìÖ Data: 2026-01-16T12:40:28.577157
üî¢ Sess√£o #: 15
üìä Janelas: 3556
‚è±Ô∏è Dura√ß√£o: 14.0s

----------------------------------------------------------------------
üéì DISTRIBUI√á√ÉO DE ALGORITMOS (META-BANDIT)
----------------------------------------------------------------------

   ENSEMBLE: 2893 sele√ß√µes (81.4%)
   UCB1: 655 sele√ß√µes (18.4%)
   EXP3: 7 sele√ß√µes (0.2%)
   THOMPSON: 1 sele√ß√µes (0.0%)

----------------------------------------------------------------------
üé∞ MULTI-ARMED BANDITS
----------------------------------------------------------------------

   THOMPSON SAMPLING:
      Atrasados: E[R]=-0.7315 (n=16659)
      Quentes: E[R]=-0.7428 (n=10165)
      Equilibrada: E[R]=-0.7404 (n=26516)

   Melhor por E[reward]: ATRASADOS

   Sele√ß√µes por estrat√©gia:
      Quentes: 1765 (49.6%)
      Atrasados: 1784 (50.2%)
      Equilibrada: 7 (0.2%)

----------------------------------------------------------------------
üî¢ FEATURE IMPORTANCE - Top N√∫meros
----------------------------------------------------------------------

   Top 10 n√∫meros mais importantes: [2, 10, 4, 13, 17, 25, 15, 1, 12, 9]
   (N√∫meros que mais contribuem para acertos)

----------------------------------------------------------------------
üìà ESTAT√çSTICAS POR ESTRAT√âGIA
----------------------------------------------------------------------

üéØ ATRASADOS
   Vezes selecionada: 1784
   ROI: -74.2%
   Reward m√©dio: -0.7426
   Custo: R$ 152953.50
   Lucro l√≠quido: R$ -113502.50
   Acertos 11+: 4505
      14 acertos: 1x
      13 acertos: 67x
      12 acertos: 721x
      11 acertos: 3716x

üéØ QUENTES
   Vezes selecionada: 1765
   ROI: -74.3%
   Reward m√©dio: -0.7437
   Custo: R$ 65838.50
   Lucro l√≠quido: R$ -48947.50
   Acertos 11+: 1987
      13 acertos: 29x
      12 acertos: 310x
      11 acertos: 1648x

üéØ EQUILIBRADA
   Vezes selecionada: 7
   ROI: -74.4%
   Reward m√©dio: -0.7444
   Custo: R$ 323659.00
   Lucro l√≠quido: R$ -240884.00
   Acertos 11+: 9706
      13 acertos: 138x
      12 acertos: 1567x
      11 acertos: 8001x

----------------------------------------------------------------------
‚öôÔ∏è HIPERPAR√ÇMETROS (Bayesian Optimization / TPE)
----------------------------------------------------------------------

   limite_atraso_minimo: 6.6603
   limite_frequencia_quente: 0.6193
   peso_atrasados_mix: 4.9422
   peso_quentes_mix: 5.7925

----------------------------------------------------------------------
üìä EVOLU√á√ÉO DO APRENDIZADO
----------------------------------------------------------------------

   ROI m√©dio atual: -74.33%
   ROI m√©dio anterior: -74.31%
   Delta: üìâ -0.01%

   ‚ö†Ô∏è Performance abaixo da sess√£o anterior.
   Meta-Bandit ir√° ajustar automaticamente.

======================================================================
üéì 10 ALGORITMOS ACAD√äMICOS IMPLEMENTADOS
======================================================================

   1. THOMPSON SAMPLING (Auer et al.)
      Converg√™ncia assint√≥tica garantida para estrat√©gia √≥tima.

   2. UCB1 - Upper Confidence Bound (Auer et al., 2002)
      Bound otimista determin√≠stico para explora√ß√£o.

   3. EXP3 - Adversarial Bandit (Auer et al., 2002)
      Robusto a mudan√ßas de distribui√ß√£o.

   4. BAYESIAN OPTIMIZATION / TPE
      Otimiza√ß√£o de hiperpar√¢metros com menos avalia√ß√µes.

   5. GENETIC ALGORITHM (Holland, 1975)
      Evolui combina√ß√µes via crossover, muta√ß√£o e sele√ß√£o.

   6. SIMULATED ANNEALING (Kirkpatrick et al., 1983)
      Escape de m√≠nimos locais via temperatura.

   7. ENSEMBLE LEARNING
      Combina votos de m√∫ltiplos algoritmos.

   8. EXPONENTIAL MOVING AVERAGE (EMA)
      Detecta mudan√ßas de tend√™ncia rapidamente.

   9. FEATURE IMPORTANCE
      Identifica n√∫meros/padr√µes mais importantes.

   10. META-BANDIT (UCB sobre algoritmos)
       Seleciona qual algoritmo usar baseado em performance.
