#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ“Š MONITOR DE VALIDAÃ‡ÃƒO DE PREDIÃ‡Ã•ES
===================================
Sistema que monitora e valida a eficÃ¡cia das prediÃ§Ãµes e calibraÃ§Ãµes
comparando com resultados reais dos concursos.

Funcionalidades:
- Registra prediÃ§Ãµes         resultado_exemplo = {
            'concurso': 3505,
            'numeros': [1, 2, 3, 4, 6, 7, 8, 9, 11, 14, 16, 20, 21, 23, 25],
            'menor_que_anterior': 11,  # CORRIGIDO - mÃ©todo posiÃ§Ã£o por posiÃ§Ã£o
            'maior_que_anterior': 0,   # CORRIGIDO - mÃ©todo posiÃ§Ã£o por posiÃ§Ã£o
            'igual': 4,                # CORRIGIDO - mÃ©todo posiÃ§Ã£o por posiÃ§Ã£o
            'soma': 170,
            'repeticoes_posicao': 4
        }lo sistema
- Compara com resultados reais quando disponÃ­veis  
- Calcula taxa de acerto de cada tipo de prediÃ§Ã£o
- Monitora eficÃ¡cia das calibraÃ§Ãµes automÃ¡ticas
- Gera relatÃ³rios de desempenho
- Aprende com erros e acertos para melhorar

Autor: AR CALHAU
Data: 06 de Outubro de 2025
"""

import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import statistics

@dataclass
class RegistroPredicao:
    """Registro de uma prediÃ§Ã£o feita pelo sistema"""
    timestamp: str
    concurso: int
    tipo_predicao: str  # 'estado_comparacao', 'soma', 'inversao', 'combinacao'
    predicao: dict
    confianca: float
    cenario_detectado: str
    parametros_calibracao: dict
    resultado_real: Optional[dict] = None
    acertou: Optional[bool] = None
    pontuacao_acerto: Optional[float] = None
    observacoes: str = ""

class MonitorValidacao:
    """Monitor principal para validaÃ§Ã£o de prediÃ§Ãµes"""
    
    def __init__(self):
        self.pasta_validacao = "validacao_predicoes"
        os.makedirs(self.pasta_validacao, exist_ok=True)
        
        self.arquivo_registros = os.path.join(self.pasta_validacao, "registros_predicoes.json")
        self.arquivo_metricas = os.path.join(self.pasta_validacao, "metricas_desempenho.json")
        
        # Carrega registros existentes
        self.registros = self._carregar_registros()
        
        print("ğŸ“Š Monitor de ValidaÃ§Ã£o de PrediÃ§Ãµes inicializado")
        print(f"ğŸ“ Pasta: {self.pasta_validacao}")
        print(f"ğŸ“ Registros carregados: {len(self.registros)}")
    
    def _carregar_registros(self) -> List[RegistroPredicao]:
        """Carrega registros existentes do arquivo"""
        if os.path.exists(self.arquivo_registros):
            try:
                with open(self.arquivo_registros, 'r', encoding='utf-8') as f:
                    dados = json.load(f)
                return [RegistroPredicao(**registro) for registro in dados]
            except Exception as e:
                print(f"âš ï¸ Erro ao carregar registros: {e}")
        return []
    
    def _salvar_registros(self):
        """Salva registros no arquivo"""
        try:
            dados = [asdict(registro) for registro in self.registros]
            with open(self.arquivo_registros, 'w', encoding='utf-8') as f:
                json.dump(dados, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ Erro ao salvar registros: {e}")
    
    def registrar_predicao_concurso_3505(self):
        """Registra nossa prediÃ§Ã£o especÃ­fica para o concurso 3505"""
        predicao_3505 = RegistroPredicao(
            timestamp=datetime.now().isoformat(),
            concurso=3505,
            tipo_predicao="reset_extremo_com_combinacoes",
            predicao={
                'menor_que_anterior_esperado': 12,
                'maior_que_anterior_esperado': 2,
                'igual_esperado': 1,
                'soma_esperada': [160, 185],
                'repeticoes_posicao_esperadas': [0, 1],
                'combinacoes_otimizadas': [
                    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],  # Radical
                    [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17],  # Equilibrada  
                    [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]   # Conservadora
                ],
                'meta_minima_acertos': 12
            },
            confianca=0.85,
            cenario_detectado="reset_extremo",
            parametros_calibracao={
                'atraso_repeticoes': 10,
                'tendencia_menor': 'decrescente',
                'estado_atual': [7, 5, 3],
                'convergencia_padroes': True
            },
            observacoes="PrediÃ§Ã£o baseada em anÃ¡lise convergente de inversÃ£o + atraso de repetiÃ§Ãµes posicionais. Meta: pelo menos 12 pontos em uma das 3 combinaÃ§Ãµes."
        )
        
        self.registros.append(predicao_3505)
        self._salvar_registros()
        
        print("ğŸ“ PREDIÃ‡ÃƒO REGISTRADA PARA CONCURSO 3505")
        print("=" * 50)
        print(f"ğŸ¯ Tipo: {predicao_3505.tipo_predicao}")
        print(f"ğŸ“ˆ ConfianÃ§a: {predicao_3505.confianca:.1%}")
        print(f"ğŸª CenÃ¡rio: {predicao_3505.cenario_detectado}")
        print(f"ğŸ² Meta mÃ­nima: {predicao_3505.predicao['meta_minima_acertos']} acertos")
        print(f"â° ValidaÃ§Ã£o: 21:00 hoje")
        print("=" * 50)
    
    def registrar_resultado_concurso(self, concurso: int, resultado: Dict):
        """Registra resultado real de um concurso"""
        # Encontra registros para este concurso
        registros_concurso = [r for r in self.registros if r.concurso == concurso]
        
        if not registros_concurso:
            print(f"âš ï¸ Nenhuma prediÃ§Ã£o encontrada para concurso {concurso}")
            return
        
        # Atualiza cada registro com o resultado
        for registro in registros_concurso:
            registro.resultado_real = resultado
            registro.acertou = self._avaliar_acerto(registro, resultado)
            registro.pontuacao_acerto = self._calcular_pontuacao(registro, resultado)
        
        self._salvar_registros()
        
        print(f"âœ… Resultado registrado para concurso {concurso}")
        print(f"ğŸ“Š PrediÃ§Ãµes atualizadas: {len(registros_concurso)}")
    
    def _avaliar_acerto(self, registro: RegistroPredicao, resultado: Dict) -> bool:
        """Avalia se uma prediÃ§Ã£o acertou baseado no tipo"""
        if registro.tipo_predicao == "reset_extremo_com_combinacoes":
            # Verifica se pelo menos uma combinaÃ§Ã£o atingiu a meta
            numeros_sorteados = resultado.get('numeros', [])
            combinacoes = registro.predicao.get('combinacoes_otimizadas', [])
            meta_minima = registro.predicao.get('meta_minima_acertos', 12)
            
            for combinacao in combinacoes:
                acertos = len(set(combinacao) & set(numeros_sorteados))
                if acertos >= meta_minima:
                    return True
            return False
        
        elif registro.tipo_predicao == "estado_comparacao":
            # Verifica campos de comparaÃ§Ã£o
            menor_esperado = registro.predicao.get('menor_que_anterior_esperado')
            menor_real = resultado.get('menor_que_anterior')
            
            if menor_esperado and menor_real:
                # Aceita margem de erro de Â±1
                return abs(menor_esperado - menor_real) <= 1
        
        elif registro.tipo_predicao == "soma":
            soma_esperada = registro.predicao.get('soma_esperada', [])
            soma_real = resultado.get('soma')
            
            if len(soma_esperada) == 2 and soma_real:
                return soma_esperada[0] <= soma_real <= soma_esperada[1]
        
        return False
    
    def _calcular_pontuacao(self, registro: RegistroPredicao, resultado: Dict) -> float:
        """Calcula pontuaÃ§Ã£o de acerto (0.0 a 1.0)"""
        if registro.tipo_predicao == "reset_extremo_com_combinacoes":
            numeros_sorteados = resultado.get('numeros', [])
            combinacoes = registro.predicao.get('combinacoes_otimizadas', [])
            
            # Calcula acertos de cada combinaÃ§Ã£o
            acertos = []
            for combinacao in combinacoes:
                acerto = len(set(combinacao) & set(numeros_sorteados))
                acertos.append(acerto / 15.0)  # Normaliza para 0-1
            
            # Retorna a melhor pontuaÃ§Ã£o
            return max(acertos) if acertos else 0.0
        
        elif registro.tipo_predicao == "estado_comparacao":
            menor_esperado = registro.predicao.get('menor_que_anterior_esperado')
            menor_real = resultado.get('menor_que_anterior')
            
            if menor_esperado and menor_real:
                erro = abs(menor_esperado - menor_real)
                return max(0.0, 1.0 - (erro / 5.0))  # PontuaÃ§Ã£o decresce com erro
        
        return 0.0
    
    def gerar_relatorio_desempenho(self) -> Dict:
        """Gera relatÃ³rio completo de desempenho"""
        if not self.registros:
            return {'erro': 'Nenhum registro disponÃ­vel'}
        
        # Filtra registros com resultado
        registros_validados = [r for r in self.registros if r.resultado_real is not None]
        
        if not registros_validados:
            return {'erro': 'Nenhum registro validado ainda'}
        
        # Calcula mÃ©tricas gerais
        total_predicoes = len(registros_validados)
        acertos = sum(1 for r in registros_validados if r.acertou)
        taxa_acerto = acertos / total_predicoes if total_predicoes > 0 else 0
        
        # PontuaÃ§Ã£o mÃ©dia
        pontuacoes = [r.pontuacao_acerto for r in registros_validados if r.pontuacao_acerto is not None]
        pontuacao_media = statistics.mean(pontuacoes) if pontuacoes else 0
        
        # Por tipo de prediÃ§Ã£o
        por_tipo = {}
        for registro in registros_validados:
            tipo = registro.tipo_predicao
            if tipo not in por_tipo:
                por_tipo[tipo] = {'total': 0, 'acertos': 0, 'pontuacoes': []}
            
            por_tipo[tipo]['total'] += 1
            if registro.acertou:
                por_tipo[tipo]['acertos'] += 1
            if registro.pontuacao_acerto is not None:
                por_tipo[tipo]['pontuacoes'].append(registro.pontuacao_acerto)
        
        # Calcula taxa por tipo
        for tipo in por_tipo:
            dados = por_tipo[tipo]
            dados['taxa_acerto'] = dados['acertos'] / dados['total'] if dados['total'] > 0 else 0
            dados['pontuacao_media'] = statistics.mean(dados['pontuacoes']) if dados['pontuacoes'] else 0
        
        relatorio = {
            'timestamp': datetime.now().isoformat(),
            'metricas_gerais': {
                'total_predicoes': total_predicoes,
                'total_acertos': acertos,
                'taxa_acerto_geral': taxa_acerto,
                'pontuacao_media_geral': pontuacao_media
            },
            'por_tipo_predicao': por_tipo,
            'registros_pendentes': len(self.registros) - len(registros_validados)
        }
        
        # Salva mÃ©tricas
        with open(self.arquivo_metricas, 'w', encoding='utf-8') as f:
            json.dump(relatorio, f, indent=2, ensure_ascii=False)
        
        return relatorio
    
    def exibir_relatorio_completo(self):
        """Exibe relatÃ³rio detalhado no console"""
        relatorio = self.gerar_relatorio_desempenho()
        
        if 'erro' in relatorio:
            print(f"âš ï¸ {relatorio['erro']}")
            return
        
        print("\n" + "=" * 70)
        print("ğŸ“Š RELATÃ“RIO DE DESEMPENHO DAS PREDIÃ‡Ã•ES")
        print("=" * 70)
        
        metricas = relatorio['metricas_gerais']
        print(f"ğŸ“ Total de prediÃ§Ãµes validadas: {metricas['total_predicoes']}")
        print(f"âœ… Total de acertos: {metricas['total_acertos']}")
        print(f"ğŸ“ˆ Taxa de acerto geral: {metricas['taxa_acerto_geral']:.1%}")
        print(f"ğŸ¯ PontuaÃ§Ã£o mÃ©dia: {metricas['pontuacao_media_geral']:.3f}")
        print(f"â³ Registros pendentes: {relatorio['registros_pendentes']}")
        
        print(f"\nğŸ“Š DESEMPENHO POR TIPO DE PREDIÃ‡ÃƒO:")
        print("-" * 50)
        
        for tipo, dados in relatorio['por_tipo_predicao'].items():
            print(f"\nğŸ¯ {tipo}:")
            print(f"   ğŸ“ Total: {dados['total']}")
            print(f"   âœ… Acertos: {dados['acertos']}")
            print(f"   ğŸ“ˆ Taxa: {dados['taxa_acerto']:.1%}")
            print(f"   ğŸ¯ PontuaÃ§Ã£o: {dados['pontuacao_media']:.3f}")
        
        print("\n" + "=" * 70)
    
    def validar_concurso_3505_exemplo(self):
        """Simula validaÃ§Ã£o do concurso 3505 com resultado exemplo"""
        # Resultado exemplo (serÃ¡ substituÃ­do pelo real Ã s 21:00)
        resultado_exemplo = {
            'concurso': 3505,
            'numeros': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],
            'menor_que_anterior': 11,
            'maior_que_anterior': 3,
            'igual': 1,
            'soma': 165,
            'repeticoes_posicao': 1
        }
        
        print("ğŸ”® SIMULAÃ‡ÃƒO DE VALIDAÃ‡ÃƒO - CONCURSO 3505")
        print("=" * 50)
        print("âš ï¸ Este Ã© um resultado EXEMPLO para teste")
        print("ğŸ“Š Resultado real serÃ¡ inserido Ã s 21:00")
        print("=" * 50)
        
        self.registrar_resultado_concurso(3505, resultado_exemplo)
        self.exibir_relatorio_completo()

def main():
    """FunÃ§Ã£o principal"""
    monitor = MonitorValidacao()
    
    # Registra prediÃ§Ã£o para 3505 se ainda nÃ£o foi registrada
    registros_3505 = [r for r in monitor.registros if r.concurso == 3505]
    if not registros_3505:
        monitor.registrar_predicao_concurso_3505()
    else:
        print("ğŸ“ PrediÃ§Ã£o para concurso 3505 jÃ¡ registrada")
    
    # Exibe status atual
    print(f"\nğŸ“Š MONITOR DE VALIDAÃ‡ÃƒO - STATUS ATUAL")
    print(f"ğŸ“ Total de registros: {len(monitor.registros)}")
    print(f"â³ Aguardando resultado do concurso 3505 Ã s 21:00")
    
    # Simula validaÃ§Ã£o exemplo
    print(f"\nğŸ§ª Executando simulaÃ§Ã£o de validaÃ§Ã£o...")
    monitor.validar_concurso_3505_exemplo()

if __name__ == "__main__":
    main()